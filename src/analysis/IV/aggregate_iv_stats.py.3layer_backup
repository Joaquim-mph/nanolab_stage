#!/usr/bin/env python3
"""
Aggregate statistics for repeated IV experiments.

For a given date and voltage range grouping, compute:
- Mean forward and backward traces
- Standard deviation per voltage point
- Fits to backward trace
- Separate results per V_max range
"""

import polars as pl
import numpy as np
from pathlib import Path
from scipy.optimize import curve_fit
import argparse
from typing import Optional
import sys

# Add project root to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent))
from src.models.parameters import IVAnalysisParameters
from pydantic import ValidationError


def linear_fit(V, a, b):
    """Linear model: I = a*V + b"""
    return a * V + b


def polynomial_fit(V, coeffs):
    """Polynomial model: I = sum(coeffs[i] * V^i)"""
    result = np.zeros_like(V)
    for i, coeff in enumerate(coeffs):
        result += coeff * (V ** i)
    return result


def separate_forward_backward(df: pl.DataFrame) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Separate forward and backward sweeps based on voltage direction.

    Assumes forward sweep goes from low to high voltage, backward goes high to low.
    Detects the turning point where dV/dt changes sign.
    """
    # Sort by voltage to find turning point
    df_sorted = df.sort("VG_V")

    # Find the midpoint index (assumes symmetric sweep)
    n = len(df_sorted)
    mid = n // 2

    forward = df_sorted[:mid]
    backward = df_sorted[mid:].sort("VG_V", descending=True)

    return forward, backward


def _process_fits_and_save(
    v_max: float,
    n_runs: int,
    forward_stats: pl.DataFrame,
    return_stats: pl.DataFrame,
    output_dir: Path,
    results: list,
    poly_orders: list[int] = [1, 3, 5, 7],
):
    """
    Process polynomial fits and save results for a given V_max group.

    This is shared logic between 3-layer and 4-layer modes.
    """
    # Get voltage and current arrays for fitting
    v_return = return_stats["V (V)"].to_numpy()
    i_return = return_stats["I_mean"].to_numpy()

    print(f"  Return segments: {len(return_stats)} voltage points")

    # Fit multiple polynomial orders
    poly_fits = {}

    for order in poly_orders:
        try:
            # Use numpy polyfit for polynomial fitting
            coeffs = np.polyfit(v_return, i_return, order)
            i_fit_poly = np.polyval(coeffs, v_return)

            # Compute R-squared
            ss_res = np.sum((i_return - i_fit_poly)**2)
            ss_tot = np.sum((i_return - np.mean(i_return))**2)
            r_squared_poly = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

            poly_fits[f"poly{order}"] = {
                "coeffs": coeffs,
                "r_squared": r_squared_poly,
                "order": order
            }

            print(f"  Polynomial order {order}: R² = {r_squared_poly:.6f}")

        except Exception as e:
            print(f"  Polynomial order {order} fit failed: {e}")
            poly_fits[f"poly{order}"] = None

    # Linear fit for backward compatibility
    try:
        popt, pcov = curve_fit(linear_fit, v_return, i_return)
        slope, intercept = popt
        slope_err, intercept_err = np.sqrt(np.diag(pcov))

        # Compute R-squared
        i_fit = linear_fit(v_return, slope, intercept)
        ss_res = np.sum((i_return - i_fit)**2)
        ss_tot = np.sum((i_return - np.mean(i_return))**2)
        r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

        print(f"  Linear fit: I = {slope:.3e} * V + {intercept:.3e}")
        print(f"    R² = {r_squared:.4f}")
        print(f"    Resistance = {1/slope:.2e} Ω")

        fit_params = {
            "v_max": v_max,
            "slope": slope,
            "slope_err": slope_err,
            "intercept": intercept,
            "intercept_err": intercept_err,
            "r_squared": r_squared,
            "resistance_ohm": 1/slope if slope != 0 else np.inf,
            "n_runs": n_runs
        }
    except Exception as e:
        print(f"  Linear fit failed: {e}")
        fit_params = {
            "v_max": v_max,
            "slope": None,
            "slope_err": None,
            "intercept": None,
            "intercept_err": None,
            "r_squared": None,
            "resistance_ohm": None,
            "n_runs": n_runs
        }

    results.append(fit_params)

    # Save per-V_max results
    v_max_clean = str(v_max).replace(".", "p")
    forward_stats.write_csv(output_dir / f"forward_vmax{v_max_clean}V.csv")
    return_stats.write_csv(output_dir / f"return_vmax{v_max_clean}V.csv")

    # Add fit columns to return stats (linear + polynomial orders)
    if fit_params["slope"] is not None:
        v_vals = return_stats["V (V)"].to_numpy()

        # Linear fit
        return_with_fit = return_stats.with_columns(
            pl.lit(linear_fit(v_vals, fit_params["slope"], fit_params["intercept"])).alias("I_fit_linear")
        )

        # Add polynomial fits
        for order in poly_orders:
            poly_key = f"poly{order}"
            if poly_fits.get(poly_key) is not None:
                coeffs = poly_fits[poly_key]["coeffs"]
                i_poly = np.polyval(coeffs, v_vals)
                return_with_fit = return_with_fit.with_columns(
                    pl.lit(i_poly).alias(f"I_fit_poly{order}")
                )

        return_with_fit.write_csv(output_dir / f"return_with_fit_vmax{v_max_clean}V.csv")

    # Save polynomial fit coefficients
    poly_summary = {
        "v_max": v_max,
        "n_runs": n_runs
    }
    for order in poly_orders:
        poly_key = f"poly{order}"
        if poly_fits.get(poly_key) is not None:
            poly_summary[f"r2_poly{order}"] = poly_fits[poly_key]["r_squared"]
            # Save coefficients as separate columns
            for i, coeff in enumerate(poly_fits[poly_key]["coeffs"][::-1]):  # Reverse for c0, c1, c2, ...
                poly_summary[f"poly{order}_c{i}"] = coeff
        else:
            poly_summary[f"r2_poly{order}"] = None

    # Store for later saving
    if not hasattr(aggregate_iv_stats, 'poly_results'):
        aggregate_iv_stats.poly_results = []
    aggregate_iv_stats.poly_results.append(poly_summary)


def load_segmented_data(
    intermediate_root: Path,
    date: str,
    procedure: str = "IV",
    chip_number: Optional[str] = None,
) -> tuple[pl.DataFrame, pl.DataFrame]:
    """
    Load pre-segmented data from intermediate layer.

    Returns forward and return segments separately.

    Args:
        intermediate_root: Root of intermediate segmented data
        date: Date in YYYY-MM-DD format
        procedure: Procedure name
        chip_number: Filter by chip number

    Returns:
        (forward_df, return_df) tuple of DataFrames
    """
    # Load all segments for the date
    segment_pattern = str(intermediate_root / f"proc={procedure}" / f"date={date}" / "run_id=*" / "segment=*" / "part-*.parquet")

    print(f"[4-layer mode] Scanning segmented data: {segment_pattern}")

    try:
        lf = pl.scan_parquet(segment_pattern)
        df_all = lf.collect()
    except Exception as e:
        print(f"Error loading segmented data: {e}")
        raise

    print(f"Found {len(df_all)} data points across {df_all['run_id'].n_unique()} runs, {df_all['segment_id'].n_unique()} segments")

    # Filter by chip number if specified
    if chip_number and "chip_number" in df_all.columns:
        df_all = df_all.filter(pl.col("chip_number") == chip_number)
        print(f"Filtered to chip {chip_number}: {df_all['run_id'].n_unique()} runs")

    # Separate forward and return segments using segment_type metadata
    # Forward segments: forward_negative, forward_positive
    # Return segments: return_negative, return_positive
    forward_df = df_all.filter(pl.col("segment_type").str.contains("forward"))
    return_df = df_all.filter(pl.col("segment_type").str.contains("return"))

    print(f"  Forward segments: {len(forward_df)} points ({forward_df['segment_id'].n_unique()} segments)")
    print(f"  Return segments: {len(return_df)} points ({return_df['segment_id'].n_unique()} segments)")

    return forward_df, return_df


def aggregate_iv_stats(
    stage_root: Path,
    date: str,
    output_dir: Path,
    procedure: str = "IVg",
    v_max_min: Optional[float] = None,
    chip_number: Optional[str] = None,
    use_segments: bool = False,
    intermediate_root: Optional[Path] = None,
):
    """
    Aggregate IV statistics for repeated experiments on a given date.

    Args:
        stage_root: Root of staged data (3-layer mode)
        date: Date in YYYY-MM-DD format
        output_dir: Where to save results
        procedure: Procedure name (IV, IVg, etc.)
        v_max_min: Filter by V_max parameter (e.g., 1.0, 2.0, ..., 8.0)
        chip_number: Filter by chip number
        use_segments: If True, read from intermediate_root (4-layer mode)
        intermediate_root: Root of intermediate segmented data (required if use_segments=True)
    """

    # 4-LAYER MODE: Read from intermediate segmented data
    if use_segments:
        if intermediate_root is None:
            raise ValueError("intermediate_root must be provided when use_segments=True")

        print("\n" + "="*80)
        print("4-LAYER MODE: Using pre-segmented data from intermediate layer")
        print("="*80)

        forward_all, return_all = load_segmented_data(
            intermediate_root=intermediate_root,
            date=date,
            procedure=procedure,
            chip_number=chip_number,
        )

        # Detect voltage column
        if "Vg (V)" in forward_all.columns:
            v_col = "Vg (V)"
        elif "Vsd (V)" in forward_all.columns:
            v_col = "Vsd (V)"
        else:
            raise ValueError(f"No voltage column found. Available columns: {forward_all.columns}")

        # Calculate max voltage for grouping from forward segments
        v_max_per_run = (
            forward_all.group_by("run_id")
            .agg(pl.col(v_col).abs().max().alias("v_max"))
        )

        forward_all = forward_all.join(v_max_per_run, on="run_id")
        return_all = return_all.join(v_max_per_run, on="run_id")

        # Round to nearest integer for grouping
        forward_all = forward_all.with_columns((pl.col("v_max").round(0)).alias("v_max_group"))
        return_all = return_all.with_columns((pl.col("v_max").round(0)).alias("v_max_group"))

        # Filter by v_max if specified
        if v_max_min is not None:
            forward_all = forward_all.filter(pl.col("v_max_group") == v_max_min)
            return_all = return_all.filter(pl.col("v_max_group") == v_max_min)
            groups = [(v_max_min, forward_all, return_all)]
            print(f"Filtering for V_max ≈ {v_max_min}V")
        else:
            # Group by V_max values
            v_max_values = sorted(forward_all["v_max_group"].unique().to_list())
            groups = [
                (
                    v_max,
                    forward_all.filter(pl.col("v_max_group") == v_max),
                    return_all.filter(pl.col("v_max_group") == v_max)
                )
                for v_max in v_max_values
            ]
            print(f"Found V_max ranges: {v_max_values}")

    # 3-LAYER MODE: Read from stage and detect segments
    else:
        print("\n" + "="*80)
        print("3-LAYER MODE: Reading from stage layer (will detect segments)")
        print("="*80)

        # Load staged IV data for the date
        ivg_pattern = str(stage_root / f"proc={procedure}" / f"date={date}" / "run_id=*" / "part-*.parquet")

        print(f"Scanning: {ivg_pattern}")

        try:
            lf = pl.scan_parquet(ivg_pattern)
            df_all = lf.collect()
        except Exception as e:
            print(f"Error loading data: {e}")
            return

        print(f"Found {len(df_all)} data points across {df_all['run_id'].n_unique()} runs")

        if len(df_all) == 0:
            print("No data found for the specified date and filters")
            return

        # Filter by chip number if specified
        if chip_number and "chip_number" in df_all.columns:
            df_all = df_all.filter(pl.col("chip_number") == chip_number)
            print(f"Filtered to chip {chip_number}: {df_all['run_id'].n_unique()} runs")

        # Detect voltage column name (IVg uses "Vg (V)", IV uses "Vsd (V)")
        if "Vg (V)" in df_all.columns:
            v_col = "Vg (V)"
        elif "Vsd (V)" in df_all.columns:
            v_col = "Vsd (V)"
        else:
            raise ValueError(f"No voltage column found. Available columns: {df_all.columns}")

        print(f"Using voltage column: {v_col}")

        # Calculate max voltage for each run
        v_max_per_run = (
            df_all.group_by("run_id")
            .agg(pl.col(v_col).max().alias("v_max"))
        )

        df_all = df_all.join(v_max_per_run, on="run_id")

        # Round to nearest integer for grouping
        df_all = df_all.with_columns((pl.col("v_max").round(0)).alias("v_max_group"))

        if v_max_min is not None:
            df_all = df_all.filter(pl.col("v_max_group") == v_max_min)
            groups_3layer = [(v_max_min, df_all)]
            print(f"Filtering for V_max ≈ {v_max_min}V: {df_all['run_id'].n_unique()} runs")
        else:
            # Group by V_max values
            v_max_values = sorted(df_all["v_max_group"].unique().to_list())
            groups_3layer = [(v_max, df_all.filter(pl.col("v_max_group") == v_max)) for v_max in v_max_values]
            print(f"Found V_max ranges: {v_max_values}")

    output_dir.mkdir(parents=True, exist_ok=True)

    # Process each V_max group
    results = []

    # Process based on mode
    if use_segments:
        # 4-LAYER MODE: Use pre-segmented data (no segment detection needed)
        for v_max, forward_group, return_group in groups:
            if forward_group.is_empty() or return_group.is_empty():
                continue

            n_runs = forward_group["run_id"].n_unique()
            print(f"\nProcessing V_max = {v_max}V ({n_runs} runs)")

            # Compute statistics grouped by voltage
            forward_stats = (
                forward_group
                .group_by(v_col)
                .agg([
                    pl.col("I (A)").mean().alias("I_mean"),
                    pl.col("I (A)").std().alias("I_std"),
                    pl.col("I (A)").count().alias("n_samples")
                ])
                .sort(v_col)
            )

            return_stats = (
                return_group
                .group_by(v_col)
                .agg([
                    pl.col("I (A)").mean().alias("I_mean"),
                    pl.col("I (A)").std().alias("I_std"),
                    pl.col("I (A)").count().alias("n_samples")
                ])
                .sort(v_col)
            )

            # Rename voltage column to standard name
            forward_stats = forward_stats.rename({v_col: "V (V)"})
            return_stats = return_stats.rename({v_col: "V (V)"})

            print(f"  Forward: {len(forward_stats)} voltage points")
            print(f"  Return: {len(return_stats)} voltage points")

            # Process fits (same for both modes)
            _process_fits_and_save(
                v_max=v_max,
                n_runs=n_runs,
                forward_stats=forward_stats,
                return_stats=return_stats,
                output_dir=output_dir,
                results=results,
            )

    else:
        # 3-LAYER MODE: Detect segments manually (original logic)
        for v_max, group_df in groups_3layer:
            if group_df.is_empty():
                continue

            n_runs = group_df["run_id"].n_unique()
            print(f"\nProcessing V_max = {v_max}V ({n_runs} runs, {len(group_df)} points)")

            # Pattern observed: 0V → -Vmax → +Vmax → ~0V
            # This gives us 3 segments based on voltage direction:
            # Segment 1: 0 → -Vmax (forward negative)
            # Segment 2: -Vmax → +Vmax (contains return negative AND forward positive)
            # Segment 3: +Vmax → 0 (return positive)
            #
            # We want the RETURN traces:
            # - Return negative: -Vmax → 0 (part of segment 2)
            # - Return positive: +Vmax → 0 (segment 3)

            all_forward_dfs = []    # Forward segments
            all_return_dfs = []     # Return segments (what we fit)

            for idx, run_id in enumerate(group_df["run_id"].unique()):
                run_df = group_df.filter(pl.col("run_id") == run_id)
                run_pd = run_df.select([v_col, "I (A)"]).to_pandas()

                v = run_pd[v_col].values
                i = run_pd["I (A)"].values

                if len(v) < 4:
                    continue

                # Find min and max voltage indices
                min_idx = np.argmin(v)
                max_idx = np.argmax(v)

                # Pattern: 0 → min → max → end
                # Segment A: 0 → min (forward negative - skip)
                # Segment B: min → max (includes return negative + forward positive)
                # Segment C: max → end (return positive - fit this)

                # Within segment B, find where it crosses zero (return vs forward)
                # Find zero crossing between min and max
                zero_cross_idx = min_idx
                for j in range(min_idx, max_idx + 1):
                    if abs(v[j]) < abs(v[zero_cross_idx]):
                        zero_cross_idx = j

                # Now we have 4 logical segments:
                # 1. Forward negative: 0 → min_idx
                fwd_neg = run_pd.iloc[:min_idx+1].copy()

                # 2. Return negative: min_idx → zero_cross_idx
                ret_neg = run_pd.iloc[min_idx:zero_cross_idx+1].copy()

                # 3. Forward positive: zero_cross_idx → max_idx
                fwd_pos = run_pd.iloc[zero_cross_idx:max_idx+1].copy()

                # 4. Return positive: max_idx → end
                ret_pos = run_pd.iloc[max_idx:].copy()

                # Collect forward segments (skip these)
                if len(fwd_neg) > 1:
                    all_forward_dfs.append(pl.from_pandas(fwd_neg))
                if len(fwd_pos) > 1:
                    all_forward_dfs.append(pl.from_pandas(fwd_pos))

            # Collect RETURN segments (fit these - the "odd" segments)
            if len(ret_neg) > 1:
                all_return_dfs.append(pl.from_pandas(ret_neg))
            if len(ret_pos) > 1:
                all_return_dfs.append(pl.from_pandas(ret_pos))

        if not all_forward_dfs or not all_return_dfs:
            print(f"  Skipping V_max={v_max}V: insufficient data")
            continue

        # Concatenate segments
        forward_all = pl.concat(all_forward_dfs)
        return_all = pl.concat(all_return_dfs)  # Both return segments (neg + pos)

        # Compute statistics grouped by voltage
        forward_stats = (
            forward_all
            .group_by(v_col)
            .agg([
                pl.col("I (A)").mean().alias("I_mean"),
                pl.col("I (A)").std().alias("I_std"),
                pl.col("I (A)").count().alias("n_samples")
            ])
            .sort(v_col)
        )

        # Compute stats for all return sweeps (both negative and positive returns)
        return_all_stats = (
            return_all
            .group_by(v_col)
            .agg([
                pl.col("I (A)").mean().alias("I_mean"),
                pl.col("I (A)").std().alias("I_std"),
                pl.col("I (A)").count().alias("n_samples")
            ])
            .sort(v_col)
        )

        # Rename voltage column to standard name for output
        forward_stats = forward_stats.rename({v_col: "V (V)"})
        return_all_stats = return_all_stats.rename({v_col: "V (V)"})

        print(f"  Forward segments: {len(forward_stats)} voltage points ({len(all_forward_dfs)} traces)")
        print(f"  Return segments: {len(return_all_stats)} voltage points ({len(all_return_dfs)} traces)")

        # Fit the return traces (segments 2 and 4: -Vmax→0 and +Vmax→0)
        v_return = return_all_stats["V (V)"].to_numpy()
        i_return = return_all_stats["I_mean"].to_numpy()

        # Fit multiple polynomial orders: 1, 3, 5, 7
        poly_fits = {}

        for order in [1, 3, 5, 7]:
            try:
                # Use numpy polyfit for polynomial fitting
                coeffs = np.polyfit(v_return, i_return, order)
                i_fit_poly = np.polyval(coeffs, v_return)

                # Compute R-squared
                ss_res = np.sum((i_return - i_fit_poly)**2)
                ss_tot = np.sum((i_return - np.mean(i_return))**2)
                r_squared_poly = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

                poly_fits[f"poly{order}"] = {
                    "coeffs": coeffs,
                    "r_squared": r_squared_poly,
                    "order": order
                }

                print(f"  Polynomial order {order}: R² = {r_squared_poly:.6f}")

            except Exception as e:
                print(f"  Polynomial order {order} fit failed: {e}")
                poly_fits[f"poly{order}"] = None

        # Linear fit (order 1) for backward compatibility
        try:
            popt, pcov = curve_fit(linear_fit, v_return, i_return)
            slope, intercept = popt
            slope_err, intercept_err = np.sqrt(np.diag(pcov))

            # Compute R-squared
            i_fit = linear_fit(v_return, slope, intercept)
            ss_res = np.sum((i_return - i_fit)**2)
            ss_tot = np.sum((i_return - np.mean(i_return))**2)
            r_squared = 1 - (ss_res / ss_tot) if ss_tot > 0 else 0

            print(f"  Linear fit: I = {slope:.3e} * V + {intercept:.3e}")
            print(f"    R² = {r_squared:.4f}")
            print(f"    Resistance = {1/slope:.2e} Ω")

            fit_params = {
                "v_max": v_max,
                "slope": slope,
                "slope_err": slope_err,
                "intercept": intercept,
                "intercept_err": intercept_err,
                "r_squared": r_squared,
                "resistance_ohm": 1/slope if slope != 0 else np.inf,
                "n_runs": n_runs
            }
        except Exception as e:
            print(f"  Linear fit failed: {e}")
            fit_params = {
                "v_max": v_max,
                "slope": None,
                "slope_err": None,
                "intercept": None,
                "intercept_err": None,
                "r_squared": None,
                "resistance_ohm": None,
                "n_runs": n_runs
            }

        results.append(fit_params)

        # Save per-V_max results
        v_max_clean = str(v_max).replace(".", "p")
        forward_stats.write_csv(output_dir / f"forward_vmax{v_max_clean}V.csv")
        return_all_stats.write_csv(output_dir / f"return_vmax{v_max_clean}V.csv")

        # Add fit columns to return stats (linear + polynomial orders)
        if fit_params["slope"] is not None:
            v_vals = return_all_stats["V (V)"].to_numpy()

            # Linear fit
            return_with_fit = return_all_stats.with_columns(
                pl.lit(linear_fit(v_vals, fit_params["slope"], fit_params["intercept"])).alias("I_fit_linear")
            )

            # Add polynomial fits
            for order in [1, 3, 5, 7]:
                poly_key = f"poly{order}"
                if poly_fits.get(poly_key) is not None:
                    coeffs = poly_fits[poly_key]["coeffs"]
                    i_poly = np.polyval(coeffs, v_vals)
                    return_with_fit = return_with_fit.with_columns(
                        pl.lit(i_poly).alias(f"I_fit_poly{order}")
                    )

            return_with_fit.write_csv(output_dir / f"return_with_fit_vmax{v_max_clean}V.csv")

        # Save polynomial fit coefficients
        poly_summary = {
            "v_max": v_max,
            "n_runs": n_runs
        }
        for order in [1, 3, 5, 7]:
            poly_key = f"poly{order}"
            if poly_fits.get(poly_key) is not None:
                poly_summary[f"r2_poly{order}"] = poly_fits[poly_key]["r_squared"]
                # Save coefficients as separate columns
                for i, coeff in enumerate(poly_fits[poly_key]["coeffs"][::-1]):  # Reverse for c0, c1, c2, ...
                    poly_summary[f"poly{order}_c{i}"] = coeff
            else:
                poly_summary[f"r2_poly{order}"] = None

        # Store for later saving
        if not hasattr(aggregate_iv_stats, 'poly_results'):
            aggregate_iv_stats.poly_results = []
        aggregate_iv_stats.poly_results.append(poly_summary)

    # Save summary of all fits
    if results:
        fit_summary = pl.DataFrame(results)
        fit_summary.write_csv(output_dir / "fit_summary.csv")
        print(f"\nSaved results to {output_dir}")
        print(fit_summary)

    # Save polynomial fit summary
    if hasattr(aggregate_iv_stats, 'poly_results') and aggregate_iv_stats.poly_results:
        poly_summary_df = pl.DataFrame(aggregate_iv_stats.poly_results)
        poly_summary_df.write_csv(output_dir / "polynomial_fits_summary.csv")
        print(f"\nSaved polynomial fits to {output_dir / 'polynomial_fits_summary.csv'}")
        # Clear for next run
        aggregate_iv_stats.poly_results = []


def run_iv_aggregation(params: IVAnalysisParameters) -> None:
    """
    Run IV aggregation with Pydantic-validated parameters.

    Args:
        params: Validated IVAnalysisParameters instance
    """
    output_dir = params.get_stats_dir()

    aggregate_iv_stats(
        stage_root=params.stage_root,
        date=params.date,
        output_dir=output_dir,
        procedure=params.procedure,
        v_max_min=params.v_max,
        chip_number=params.chip_number,
    )


def main():
    parser = argparse.ArgumentParser(
        description="Aggregate IV statistics for repeated experiments",
        epilog="""
Examples:
  # Using JSON config (recommended)
  python aggregate_iv_stats.py --config config/analysis_config.json

  # Using command-line arguments (legacy)
  python aggregate_iv_stats.py \\
    --stage-root data/02_stage/raw_measurements \\
    --date 2025-09-11 \\
    --output-base-dir data/04_analysis \\
    --procedure IV \\
    --poly-orders 1 3 5 7
        """
    )

    # Pydantic mode
    parser.add_argument("--config", type=Path, help="Path to JSON configuration file (Pydantic mode)")

    # Legacy/direct arguments
    parser.add_argument("--stage-root", type=Path, help="Root of staged data")
    parser.add_argument("--date", help="Date in YYYY-MM-DD format")
    parser.add_argument("--output-base-dir", type=Path, help="Base output directory for analysis results")
    parser.add_argument("--output-dir", type=Path, help="(DEPRECATED: use --output-base-dir) Direct output directory")
    parser.add_argument("--procedure", type=str, default="IV", help="Procedure name (IV, IVg, etc.)")
    parser.add_argument("--v-max", type=float, help="Filter by specific V_max value")
    parser.add_argument("--chip-number", help="Filter by chip number")
    parser.add_argument("--poly-orders", nargs="+", type=int, default=[1, 3, 5, 7], help="Polynomial orders to fit")

    args = parser.parse_args()

    try:
        # Mode 1: JSON config file (Pydantic)
        if args.config:
            print(f"[info] Loading configuration from {args.config}")
            params = IVAnalysisParameters.model_validate_json(args.config.read_text())
            print("[info] Configuration validated successfully")

        # Mode 2: Command-line arguments
        elif args.stage_root and args.date:
            print("[info] Using command-line arguments (creating Pydantic parameters)")

            # Handle deprecated --output-dir
            if args.output_dir and not args.output_base_dir:
                print("[warning] --output-dir is deprecated, using as output_base_dir")
                output_base = args.output_dir.parent if args.output_dir.name in ["iv_stats", "hysteresis"] else args.output_dir
            else:
                output_base = args.output_base_dir or Path("data/04_analysis")

            params = IVAnalysisParameters(
                stage_root=args.stage_root,
                date=args.date,
                output_base_dir=output_base,
                procedure=args.procedure,
                v_max=args.v_max,
                chip_number=args.chip_number,
                poly_orders=args.poly_orders,
            )

        else:
            parser.print_help()
            print("\n[error] Must provide either --config or (--stage-root and --date)")
            sys.exit(1)

        # Run aggregation with validated parameters
        run_iv_aggregation(params)

    except ValidationError as e:
        print(f"\n[error] Parameter validation failed:")
        print(e)
        sys.exit(1)
    except Exception as e:
        print(f"\n[error] {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
